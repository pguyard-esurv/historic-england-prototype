#!/usr/bin/env python3
"""
Create a visual summary of API vs Scraped data comparison
"""

import json


def create_comparison_summary():
    """Create a visual summary of the data comparison"""
    print("ğŸ›ï¸  API vs Scraped Data - Comprehensive Comparison")
    print("=" * 70)
    
    try:
        with open('api_vs_scrape_comparison.json', 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        print(f"\nğŸ“Š BUILDING ANALYZED: {data['building_name']}")
        print(f"   List Entry: {data['list_entry']}")
        print(f"   Comparison Date: {data['comparison_date']}")
        
        # API Data Summary
        api_data = data['api_data']
        print(f"\nğŸ“‹ API DATA SUMMARY:")
        print("-" * 50)
        print(f"  âœ… Building Name: {api_data['name']}")
        print(f"  âœ… Grade: {api_data['grade']}")
        print(f"  âœ… List Entry: {api_data['list_entry']}")
        print(f"  âœ… Coordinates: {api_data['ngr']} (Easting: {api_data['easting']}, Northing: {api_data['northing']})")
        print(f"  âœ… List Date: {api_data['list_date']}")
        print(f"  âœ… Capture Scale: {api_data['capture_scale']}")
        print(f"  âœ… Direct Link: {api_data['hyperlink']}")
        
        # Scraped Data Summary
        scraped_data = data['scraped_data']
        if scraped_data:
            print(f"\nğŸ•·ï¸  SCRAPED DATA SUMMARY:")
            print("-" * 50)
            
            # Content counts
            total_headings = 0
            total_paragraphs = 0
            total_images = 0
            total_links = 0
            
            for tab_name, tab_data in scraped_data['tab_content'].items():
                if isinstance(tab_data, dict) and 'tab_name' in tab_data:
                    total_headings += len(tab_data.get('headings', []))
                    total_paragraphs += len(tab_data.get('paragraphs', []))
                    total_images += len(tab_data.get('images', []))
                    total_links += len(tab_data.get('links', []))
            
            print(f"  ğŸ“„ Content Extracted:")
            print(f"    - Headings: {total_headings}")
            print(f"    - Paragraphs: {total_paragraphs}")
            print(f"    - Images: {total_images}")
            print(f"    - Links: {total_links}")
            print(f"    - Tabs Processed: {scraped_data['tabs_processed']}")
            
            # Specific architectural data
            if 'official_list_entry' in scraped_data['tab_content']:
                official = scraped_data['tab_content']['official_list_entry']
                if 'specific_data' in official and official['specific_data']:
                    print(f"\n  ğŸ—ï¸  Architectural Details:")
                    for key, value in official['specific_data'].items():
                        if value:
                            if key == 'architectural_details':
                                preview = value[:100] + '...' if len(value) > 100 else value
                                print(f"    - {key}: {preview}")
                            else:
                                print(f"    - {key}: {value}")
        
        # Data Enrichment Analysis
        print(f"\nğŸ“ˆ DATA ENRICHMENT ANALYSIS:")
        print("-" * 50)
        
        api_fields = set(api_data.keys())
        scraped_fields = set()
        
        if scraped_data and 'official_list_entry' in scraped_data['tab_content']:
            official = scraped_data['tab_content']['official_list_entry']
            if 'specific_data' in official:
                scraped_fields = set(official['specific_data'].keys())
        
        common_fields = api_fields.intersection(scraped_fields)
        api_only = api_fields - scraped_fields
        scraped_only = scraped_fields - api_fields
        
        print(f"  ğŸ”„ Common Fields ({len(common_fields)}):")
        for field in sorted(common_fields):
            print(f"    - {field}")
        
        print(f"\n  ğŸ“Š API Only Fields ({len(api_only)}):")
        for field in sorted(api_only):
            print(f"    - {field}")
        
        print(f"\n  ğŸ•·ï¸  Scraped Only Fields ({len(scraped_only)}):")
        for field in sorted(scraped_only):
            print(f"    - {field}")
        
        # Value Proposition Analysis
        print(f"\nğŸ’¡ VALUE PROPOSITION ANALYSIS:")
        print("-" * 50)
        
        print(f"  ğŸ¯ API STRENGTHS:")
        print(f"    âœ… Structured, reliable data")
        print(f"    âœ… Precise coordinates (Easting/Northing)")
        print(f"    âœ… Complete building inventory (379,604+ buildings)")
        print(f"    âœ… Fast, programmatic access")
        print(f"    âœ… Consistent data format")
        print(f"    âœ… Direct links to detailed pages")
        
        print(f"\n  ğŸ¯ SCRAPING STRENGTHS:")
        print(f"    âœ… Rich, detailed descriptions")
        print(f"    âœ… Architectural details and materials")
        print(f"    âœ… Historical context and significance")
        print(f"    âœ… Images and visual content")
        print(f"    âœ… User-generated content (comments/photos)")
        print(f"    âœ… Complete listing text")
        print(f"    âœ… Construction periods and styles")
        
        print(f"\n  ğŸ¯ COMBINED VALUE:")
        print(f"    ğŸ† Complete building profiles")
        print(f"    ğŸ† Both structured and narrative data")
        print(f"    ğŸ† Geographic precision + rich context")
        print(f"    ğŸ† Scalable data collection")
        print(f"    ğŸ† Research and analysis ready")
        
        # Use Case Recommendations
        print(f"\nğŸ¯ RECOMMENDED USE CASES:")
        print("-" * 50)
        
        print(f"  ğŸ“Š API ONLY - When you need:")
        print(f"    - Building counts and statistics")
        print(f"    - Geographic analysis and mapping")
        print(f"    - Bulk data processing")
        print(f"    - Building identification and basic info")
        print(f"    - Fast, reliable data access")
        
        print(f"\n  ğŸ•·ï¸  SCRAPING ONLY - When you need:")
        print(f"    - Detailed architectural research")
        print(f"    - Historical context and significance")
        print(f"    - Visual content and images")
        print(f"    - Complete building descriptions")
        print(f"    - User-generated content")
        
        print(f"\n  ğŸ† COMBINED APPROACH - When you need:")
        print(f"    - Comprehensive building databases")
        print(f"    - Research and analysis platforms")
        print(f"    - Heritage management systems")
        print(f"    - Educational and cultural applications")
        print(f"    - Complete building profiles")
        
        # Technical Considerations
        print(f"\nâš™ï¸  TECHNICAL CONSIDERATIONS:")
        print("-" * 50)
        
        print(f"  ğŸ“Š API:")
        print(f"    - Speed: Very fast (milliseconds)")
        print(f"    - Reliability: High (99.9%+ uptime)")
        print(f"    - Rate Limits: 1000 records per query")
        print(f"    - Maintenance: None (managed by Historic England)")
        print(f"    - Cost: Free")
        
        print(f"\n  ğŸ•·ï¸  Scraping:")
        print(f"    - Speed: Slower (seconds per building)")
        print(f"    - Reliability: Medium (depends on website changes)")
        print(f"    - Rate Limits: Respectful delays needed")
        print(f"    - Maintenance: Regular updates needed")
        print(f"    - Cost: Server resources + ChromeDriver")
        
        print(f"\n  ğŸ† Combined:")
        print(f"    - Speed: Optimized (API for bulk, scraping for details)")
        print(f"    - Reliability: High (API fallback)")
        print(f"    - Rate Limits: Balanced approach")
        print(f"    - Maintenance: Moderate (scraping updates)")
        print(f"    - Cost: Efficient resource usage")
        
        print(f"\n" + "=" * 70)
        print(f"âœ… COMPARISON COMPLETE - Both data sources provide unique value!")
        print(f"   API: Structured, fast, reliable, comprehensive")
        print(f"   Scraping: Detailed, rich, contextual, visual")
        print(f"   Combined: Complete heritage data solution")
        
    except FileNotFoundError:
        print("âŒ Comparison file not found. Run api_vs_scrape_comparison.py first.")
    except Exception as e:
        print(f"âŒ Error creating summary: {e}")

if __name__ == "__main__":
    create_comparison_summary()
